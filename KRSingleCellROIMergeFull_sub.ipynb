{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import aicsimageio as aio\n",
    "from omero.gateway import BlitzGateway\n",
    "import os\n",
    "import json\n",
    "tmp_path = '/tmp/'\n",
    "import time\n",
    "tot = time.time()\n",
    "import sys\n",
    "import omero\n",
    "sys.path.append('/home/staffan/Code_KR/utils/')\n",
    "import xmltodict\n",
    "import aicsimageio as aio \n",
    "#from aicsimageio import AICSImage, imread\n",
    "import shapely as shp\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_pol_area(roi_points):\n",
    "\n",
    "    polygon = Polygon(roi_points)\n",
    "\n",
    "    return polygon.area\n",
    "\n",
    "def get_ome_anno(img_id,host=\"localhost\",show=False,user='john',pwd=\"11235813\"):\n",
    "    conn = BlitzGateway(user,pwd , host=host, port=4064)\n",
    "    conn.connect()\n",
    "    conn.c.enableKeepAlive(300)\n",
    "    \n",
    "\n",
    "    roi_service = conn.getRoiService()\n",
    "    #print(roi_service)\n",
    "    result = roi_service.findByImage(img_id, None)\n",
    "    #print(result)\n",
    "\n",
    "    roi_json = {}\n",
    "\n",
    "    for roi in result.rois:\n",
    "        print('1')\n",
    "        for s in roi.copyShapes():\n",
    "            shape = {}\n",
    "            shape['id'] = s.getId().getValue()\n",
    "            shape['theT'] = s.getTheT().getValue()\n",
    "            shape['theZ'] = s.getTheZ().getValue()\n",
    "            if s.getTextValue():\n",
    "                s1=s.getTextValue().getValue().replace('/', '_')\n",
    "                shape['textValue'] = s1\n",
    "            if type(s) == omero.model.RectangleI:\n",
    "                shape['type'] = 'Rectangle'\n",
    "                shape['x'] = s.getX().getValue()\n",
    "                shape['y'] = s.getY().getValue()\n",
    "                shape['width'] = s.getWidth().getValue()\n",
    "                shape['height'] = s.getHeight().getValue()\n",
    "            elif type(s) == omero.model.EllipseI:\n",
    "                shape['type'] = 'Ellipse'\n",
    "                shape['x'] = s.getX().getValue()\n",
    "                shape['y'] = s.getY().getValue()\n",
    "                shape['radiusX'] = s.getRadiusX().getValue()\n",
    "                shape['radiusY'] = s.getRadiusY().getValue()\n",
    "            elif type(s) == omero.model.PointI:\n",
    "                shape['type'] = 'Point'\n",
    "                shape['x'] = s.getX().getValue()\n",
    "                shape['y'] = s.getY().getValue()\n",
    "            elif type(s) == omero.model.LineI:\n",
    "                shape['type'] = 'Line'\n",
    "                shape['x1'] = s.getX1().getValue()\n",
    "                shape['x2'] = s.getX2().getValue()\n",
    "                shape['y1'] = s.getY1().getValue()\n",
    "                shape['y2'] = s.getY2().getValue()\n",
    "            elif type(s) == omero.model.MaskI:\n",
    "                shape['type'] = 'Mask'\n",
    "                shape['x'] = s.getX().getValue()\n",
    "                shape['y'] = s.getY().getValue()\n",
    "                shape['width'] = s.getWidth().getValue()\n",
    "                shape['height'] = s.getHeight().getValue()\n",
    "            elif type(s) == omero.model.PolygonI:\n",
    "                #print('polygon')\n",
    "                shape['type'] = 'Polygon'\n",
    "                shape['points'] =[[int(float(i.split(',')[0])),int(float(i.split(',')[1]))] for i in s.getPoints().getValue().split(' ')]\n",
    "                shape['area']= get_pol_area(shape['points'])\n",
    "                \n",
    "                #KR added in area to use for normalisation further down the line\n",
    "            \n",
    "                #shape['points'] = s.getPoints().getValue()\n",
    "            elif type(s) in (\n",
    "                    omero.model.LabelI, ):\n",
    "                print(type(s), \" Not supported by this code\")\n",
    "            # Do some processing here, or just print:\n",
    "            if show:\n",
    "                print(\"   Shape:\",)\n",
    "                for key, value in shape.items():\n",
    "                    print( \"  \", key, value,)\n",
    "                \n",
    "\n",
    "            roi_json[roi.getId().getValue()] = shape\n",
    "            \n",
    "\n",
    "\n",
    "    conn.close()\n",
    "    \n",
    "    return roi_json\n",
    "\n",
    "def get_image_path(imageId,host='localhost',user='john',pwd=\"11235813\"):\n",
    "    \"\"\"\n",
    "    return the path location\n",
    "    \"\"\"\n",
    "    conn = BlitzGateway(user,pwd , host=host, port=4064)\n",
    "    conn.connect()\n",
    "    image = conn.getObject(\"Image\", imageId)\n",
    "    tpath = 'data/OMERO/ManagedRepository/'+image.getImportedImageFilePaths()['server_paths'][0]\n",
    "    #print(image.getImportedImageFilePaths()['server_paths'])\n",
    "    print(tpath)\n",
    "\n",
    "    conn.close()\n",
    "    # print(host,host=='192.168.193.15',tpath)\n",
    "    if host in ['192.168.193.15','172.27.107.136','localhost']: \n",
    "        if os.path.exists('/'+ tpath):\n",
    "            print('Found the images')\n",
    "            return '/'+tpath\n",
    "\n",
    "    print('copying the whole slide image to localserver, please remember to remove that after use')\n",
    "    dst_path = '%s%s'%(tmp_path,tpath.split('/')[-1])\n",
    "    if not os.path.exists(dst_path):\n",
    "        tflag= os.system('scp john-imac:/%s %s'%(tpath,tmp_path))\n",
    "    else:\n",
    "        return dst_path\n",
    "    if tflag ==0 and os.path.exists(dst_path):\n",
    "        return dst_path\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_meta_path(imageId, project, subproject, outputdir, host='localhost',user='john',pwd=\"11235813\"):\n",
    "    conn = BlitzGateway(user,pwd , host=host, port=4064)\n",
    "    conn.connect()\n",
    "    conn.c.enableKeepAlive(300)\n",
    "    image = conn.getObject(\"Image\", imageId)\n",
    "    tpath = image.getImportedImageFilePaths()['server_paths'][0]\n",
    "    tcpath = image.getImportedImageFilePaths()['client_paths'][0]\n",
    "    dataset=image.getParent()\n",
    "    datasetname=dataset.getName()\n",
    "    print('dataset is:', datasetname, 'json is')\n",
    "    ann = dataset.getAnnotation()\n",
    "    ann_list=dataset.listAnnotations()\n",
    "    \n",
    "    project=project\n",
    "    directory = os.path.join(outputdir,project,subproject,'Ref/')\n",
    "    for folder in os.listdir(directory):\n",
    "        if folder[:4]==datasetname:\n",
    "            meta_path = directory+folder+'/Images/FieldTables.json'\n",
    "            print('metapath found', meta_path)\n",
    "            return meta_path\n",
    "    \n",
    "      \n",
    "\n",
    "def get_cluster_id(target_size,filepath, json_file):\n",
    "    \"\"\"\n",
    "    Get cluster id based on image size\n",
    "    ## Doesn't work, need to based on image name as well\n",
    "    \"\"\"\n",
    "    \n",
    "    tsx,tsy = target_size\n",
    "    \n",
    "    ta = json.loads(open(json_file,'r').read())\n",
    "    if filepath is not None:\n",
    "        filename = filepath.split('/')[-1]\n",
    "        \n",
    "        \n",
    "        tcluster = int(filename.split('_F')[1].split('P')[0])-1\n",
    "        \n",
    "            #print(filename,tcluster)\n",
    "        \n",
    "        if ta[0]['FieldTables'][tcluster]['Dims'][0]==tsy and ta[0]['FieldTables'][tcluster]['Dims'][1]==tsx:\n",
    "            return tcluster\n",
    "        elif ta[0]['FieldTables'][tcluster]['Dims'][0]==tsx and ta[0]['FieldTables'][tcluster]['Dims'][1]==tsy:\n",
    "            \n",
    "            return tcluster\n",
    "        \n",
    "    \n",
    "    #print(len(ta[0]['FieldTables']))\n",
    "    for tcluster  in range(len(ta[0]['FieldTables'])):\n",
    "        #print(ta[0]['FieldTables'][tcluster]['Dims'])\n",
    "        if ta[0]['FieldTables'][tcluster]['Dims'][0]==tsx and ta[0]['FieldTables'][tcluster]['Dims'][1]==tsy:\n",
    "            return tcluster\n",
    "    tsy,tsx = target_size\n",
    "    ta = json.loads(open(json_file,'r').read())\n",
    "    #print(len(ta[0]['FieldTables']))\n",
    "    for tcluster  in range(len(ta[0]['FieldTables'])):\n",
    "        #print(ta[0]['FieldTables'][tcluster]['Dims'])\n",
    "        if ta[0]['FieldTables'][tcluster]['Dims'][0]==tsx and ta[0]['FieldTables'][tcluster]['Dims'][1]==tsy:\n",
    "            return tcluster\n",
    "    \n",
    "\n",
    "def get_target_field(fcenters,tclusterid,json_file):\n",
    "    \"\"\"\n",
    "    input = index of the fcenters\n",
    "    return targeted field, offsetx, offsety in the tile, startx, starty in the wsi\n",
    "    \"\"\"\n",
    "    cell_x,cell_y = fcenters\n",
    "    #print('roi points:', cell_x,cell_y)\n",
    "    \n",
    "    ta = json.loads(open(json_file,'r').read())\n",
    "    \n",
    "    tfield_num = len(ta[0]['FieldTables'][tclusterid]['FieldTable']['Field'])\n",
    "    #print('tfield_num is:', tfield_num)\n",
    "    binning =ta[0]['FieldTables'][0]['Binning']\n",
    "    for tindex in range(tfield_num):\n",
    "        #loops all fields in the cluster (one field is 1987 pixels)\n",
    "    \n",
    "        #print(ta[0]['FieldTables'][tcluster]['FieldTable']['X'][tindex])\n",
    "        tfx = ta[0]['FieldTables'][tclusterid]['FieldTable']['X'][tindex]\n",
    "\n",
    "        tfy = ta[0]['FieldTables'][tclusterid]['FieldTable']['Y'][tindex]\n",
    "        #print('tfx and tfy from json file:', tfx,tfy)\n",
    "        if cell_x >tfx and cell_x < tfx+(2160//binning) and cell_y > tfy and cell_y <tfy+(2160//binning):\n",
    "            #print('ROI within bounds of field given', ta[0]['FieldTables'][tclusterid]['FieldTable']['Field'][tindex])\n",
    "            #if roi x,y is lower than field coords then it is not in field. \n",
    "            \n",
    "                #return tindex\n",
    "            #print('tfx and tfy from json file:', tfx,tfy)\n",
    "                #ffa.append([tindex,tfx,tfy]) targetfieldId,txx,tyy,ofx,ofy\n",
    "            #print('the output of get target field is:', 'field ID:', ta[0]['FieldTables'][tclusterid]['FieldTable']['Field'][tindex], 'txx:',\n",
    "                     #cell_x-tfx, 'txy:', cell_y-tfy,'ofx', tfx,'ofy', tfy)\n",
    "\n",
    "            return ta[0]['FieldTables'][tclusterid]['FieldTable']['Field'][tindex],\\\n",
    "                     cell_x-tfx, cell_y-tfy,tfx,tfy\n",
    "    \n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "def get_xml_ref(imageId, project, subproject, outputdir):\n",
    "    conn = BlitzGateway(user, password , host=host, port=4064)\n",
    "    conn.connect()\n",
    "    image = conn.getObject(\"Image\", imageId)\n",
    "    tpath = image.getImportedImageFilePaths()['server_paths'][0]\n",
    "    tcpath = image.getImportedImageFilePaths()['client_paths'][0]\n",
    "    dataset=image.getParent()\n",
    "    datasetname=dataset.getName()\n",
    "    ann = dataset.getAnnotation()\n",
    "    ann_list=dataset.listAnnotations()\n",
    "    print(datasetname)\n",
    "    \n",
    "    \n",
    "    directory = os.path.join(outputdir, project,subproject, 'Ref/')\n",
    "    for folder in os.listdir(directory):\n",
    "        if folder[:4]==datasetname:\n",
    "            xml_path = directory+folder+'/Images/Index.ref.xml'\n",
    "            #print('xml path found', xml_path)\n",
    "            return xml_path \n",
    "    \n",
    "def get_field_position_old(targetfield,parsed_xml):\n",
    "    # The below means I don't need to find and replace tdict with parsed_xml when rewriting this function. \n",
    "    tdict = parsed_xml\n",
    "    for i  in range(len(tdict['EvaluationInputData']['Wells']['Well']['Image'])):\n",
    "        tff = int(tdict['EvaluationInputData']['Wells']['Well']['Image'][i]['@id'].split('P')[0].split('F')[1])\n",
    "        if tff == targetfield and int(tdict['EvaluationInputData']['Images']['Image'][i]['FieldID'])==targetfield:\n",
    "            #print('get field position uses xml data from harmony, goes through all images (fields, i think), turns image name into tff(', tff, ') if tff is same as target field(', targetfield, ') from json cluster field, then it returns obx:', float(tdict['EvaluationInputData']['Images']['Image'][i]['PositionX']['#text']), ', oby:', float(tdict['EvaluationInputData']['Images']['Image'][i]['PositionY']['#text']), ', and res:', float(tdict['EvaluationInputData']['Images']['Image'][i]['ImageResolutionX']['#text']))\n",
    "            #print(i)\n",
    "            return float(tdict['EvaluationInputData']['Images']['Image'][i]['PositionX']['#text']),\\\n",
    "                    float(tdict['EvaluationInputData']['Images']['Image'][i]['PositionY']['#text']),\\\n",
    "                    float(tdict['EvaluationInputData']['Images']['Image'][i]['ImageResolutionX']['#text'])\n",
    "    return None \n",
    "\n",
    "def get_field_position(targetfield,ref_xml): ## 58s\n",
    "    fd=open(ref_xml)\n",
    "    tdict = xmltodict.parse(fd.read())\n",
    "    tlist=[int(tdict['EvaluationInputData']['Wells']['Well']['Image'][i]['@id'].split('P')[0].split('F')[1]) \\\n",
    "                   for i in range(len(tdict['EvaluationInputData']['Wells']['Well']['Image']))]\n",
    "    ti = tlist.index(targetfield)\n",
    "    if int(tdict['EvaluationInputData']['Images']['Image'][ti]['FieldID'])==targetfield:\n",
    "        #print(i)\n",
    "        return float(tdict['EvaluationInputData']['Images']['Image'][ti]['PositionX']['#text']),\\\n",
    "                float(tdict['EvaluationInputData']['Images']['Image'][ti]['PositionY']['#text']),\\\n",
    "                float(tdict['EvaluationInputData']['Images']['Image'][ti]['ImageResolutionX']['#text'])\n",
    "\n",
    "\n",
    "#     for i  in range(len(tdict['EvaluationInputData']['Wells']['Well']['Image'])):\n",
    "#         tff = int(tdict['EvaluationInputData']['Wells']['Well']['Image'][i]['@id'].split('P')[0].split('F')[1])\n",
    "#         if tff == targetfield and int(tdict['EvaluationInputData']['Images']['Image'][i]['FieldID'])==targetfield:\n",
    "#             #print(i)\n",
    "#             return float(tdict['EvaluationInputData']['Images']['Image'][i]['PositionX']['#text']),\\\n",
    "#                     float(tdict['EvaluationInputData']['Images']['Image'][i]['PositionY']['#text']),\\\n",
    "#                     float(tdict['EvaluationInputData']['Images']['Image'][i]['ImageResolutionX']['#text'])\n",
    "    return None\n",
    "\n",
    "def convert_cords(tps,ref_xml,tclusterid, json_file):\n",
    "    \"\"\"\n",
    "    input points [[tpx,tpy]]\n",
    "    return the transformed cordinates\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for tp in tps:\n",
    "        #a[0]['FieldTables'][tclusterid]['FieldTable']['Field'][tindex], cell_x-tfx, cell_y-tfy,tfx,tfy\n",
    "        \n",
    "        tpx,tpy = tp\n",
    "        targetfieldId,txx,tyy,ofx,ofy = get_target_field([tpx, tpy],tclusterid,json_file)\n",
    "        \n",
    "        obx,oby,res = get_field_position_old(targetfieldId,ref_xml)\n",
    "        results.append([(obx+res*txx-1080*res)*1000000, (oby-res*tyy+0*res+1080*res)*1000000])\n",
    "        #print('the results from get target field and get field position are then put into a list containing obx(xml)+res(xml)*txx(omero_x-json_x)*1 million,,, and oby-res*tyy+0*res+1080*res*1000000. results list is:', results)\n",
    "    return results\n",
    "\n",
    "def segment_data(user, password,imagename, imageId,rawdata, csv_object_reference):\n",
    "\n",
    "\n",
    "# Get meta data from OMERO¶\n",
    "    print('Getting metadata from OMERO...')\n",
    "    st = time.time()\n",
    "    tanno = get_ome_anno(imageId,user=user,pwd=password)\n",
    "    #print(imageId, tanno)\n",
    "    \n",
    "    dst = get_image_path(imageId,host='localhost',user=user,pwd=password)\n",
    "    dst_json = get_meta_path(imageId,project, subproject, outputdir, host='localhost',user=user,pwd=password)\n",
    "    print(time.time()-st)\n",
    "    # print('metapath is:', dst_json, 'image path is:', dst)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Reading target image, determine pixel dimensions. Used to identify witch cluster.\n",
    "    print('Reading images, identifying clusters (sections)...')\n",
    "    timg = aio.imread(dst)\n",
    "    # print(timg.shape)\n",
    "    tclusterid= get_cluster_id(timg.shape[-2:], dst, dst_json)\n",
    "    \n",
    "    ref_xml = get_xml_ref(imageId, project, subproject, outputdir)\n",
    "    print('ref xml is:', ref_xml)\n",
    "    print(time.time()-st)\n",
    "    print('...Done')\n",
    "\n",
    "\n",
    "    # Load OMERO ROI coordinates - convert to Harmony scale\n",
    "    print('Converting ROI coordinates...')\n",
    "    st = time.time()\n",
    "\n",
    "    dftanno = pd.DataFrame(tanno)\n",
    "    #print('omero image shape data:', dftanno)\n",
    "   \n",
    "    sertanno =  dftanno.loc['points', :]\n",
    "   \n",
    "    ROI_li = sertanno.tolist()\n",
    "    #print('tps is each object in Roi_li, this comes from omero shape ROI data', ROI_li)\n",
    "   \n",
    "    ROI_area=dftanno.loc['area']\n",
    "    \n",
    "    \n",
    "    ROI_Area=ROI_area.tolist()\n",
    "\n",
    "\n",
    "    # Load ROI names\n",
    "    ROInames = dftanno.loc['textValue']\n",
    "    \n",
    "    ROI_names = ROInames.tolist()\n",
    "   \n",
    "\n",
    "    cROI_li = list()\n",
    "    # This reads the XML file once, and then we use that in the loop\n",
    "    # Rather than opening it a bazillion times. Should save a lot of time.\n",
    "    fd=open(ref_xml)\n",
    "    ref_xml = xmltodict.parse(fd.read())\n",
    "    for i in ROI_li:\n",
    "        #area_list.append(pol_area)\n",
    "        # convert each set of coords.\n",
    "        cROI = convert_cords(i,ref_xml,tclusterid, json_file=dst_json)\n",
    "        \n",
    "        cROI_li.append(cROI)\n",
    "        \n",
    "\n",
    "    print(time.time()-st)\n",
    "    print('All Done!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Segment data\n",
    "    # Create Bounding box for combined polygons in one cluster. \n",
    "    st = time.time()\n",
    "    pli = list()\n",
    "    for i in range(len(cROI_li)):\n",
    "        pli.append(Polygon((cROI_li[i])))\n",
    "        #print(i)\n",
    "    bbox = MultiPolygon(pli)\n",
    "    bbox.bounds # Bounding box for mulipolygon\n",
    "    # minx, miny, maxx, maxy \n",
    "    print(time.time()-st)\n",
    "\n",
    "    # Load analysis results - filter away all cells outside the ROIs of this image.\n",
    "    st = time.time()\n",
    "    pf = pd.read_csv(rawdata, header=8, delimiter='\\t')\n",
    "    \n",
    "    \n",
    "    # pf # pandaframe\n",
    "    # data loaded here is for entire slide!\n",
    "\n",
    "    # This slices away data from other clusters.\n",
    "    # How to Select Rows of Pandas Dataframe Based on a Single Value of a Column?\n",
    "    pf.shape\n",
    "    fpf = pf\n",
    "    is_ymin = fpf['Position Y [µm]'] > bbox.bounds[1] # Y = 27852.946882244712\n",
    "    fpf = fpf[is_ymin]\n",
    "    is_ymax = fpf['Position Y [µm]'] < bbox.bounds[3] # Y = 28809.233162833487\n",
    "    fpf = fpf[is_ymax]\n",
    "    is_xmin = fpf['Position X [µm]'] > bbox.bounds[0] # X = 888.0532437902484\n",
    "    fpf = fpf[is_xmin]\n",
    "    is_xmax = fpf['Position X [µm]'] < bbox.bounds[2] # X = 2489.3144912603493\n",
    "    fpf = fpf[is_xmax]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # fpf # filtered pandaframe\n",
    "\n",
    "    fpf['ROI'] = 'not_assigned'\n",
    "    # Add column for ROI-attribute to the filtered dataframe.\n",
    "\n",
    "    # Johns way of doing it in a one liner\n",
    "    # fpff = pf.loc[(pf['Position Y [µm]']>bbox.bounds[1])&(pf['Position Y [µm]']<bbox.bounds[3])&(pf['Position X [µm]']>bbox.bounds[0])&(pf['Position X [µm]']<bbox.bounds[2])]\n",
    "    # print(fpff.shape)\n",
    "\n",
    "    # I could slice the data down to each polygons bbox. Might improve speed if .contains(coords) is slow.\n",
    "    print(time.time()-st)\n",
    "\n",
    "\n",
    "    # Segmentation.\n",
    "\n",
    "    st = time.time()\n",
    "    # fpf = filtered data (inside bounding box of the combined ROIs.)\n",
    "    # pli = the multipolygon = a list of all individual ROIs.\n",
    "\n",
    "    ##### Currently can only insert into the filtered data. But the file has maintained the original Index it seems.\n",
    "\n",
    "    for i in range(len(fpf)) :     \n",
    "        # print(fpf.iloc[i, 8], fpf.iloc[i, 9])\n",
    "        x = fpf.iloc[i, 8]\n",
    "        y = fpf.iloc[i, 9]\n",
    "        coord = [x,y]\n",
    "    #     print(coord)\n",
    "        cord = Point(coord)\n",
    "        \n",
    "    #     print(bbox.contains(cord))\n",
    "    #     print(pli[1].contains(cord))\n",
    "    #     if pli[2].contains(cord)==True:\n",
    "    #          print(coord)\n",
    "        for a in range(len(pli)) :   \n",
    "    #         print(a)\n",
    "            if pli[a].contains(cord)==True:\n",
    "    #             print(coord) # The coord\n",
    "    #             print(ROI_names[a]) # The ROI name\n",
    "    #             print(fpf.iloc[i,-1]) # The original index\n",
    "                ROI = ROI_names[a]\n",
    "                \n",
    "    #             print(ROI)\n",
    "    #             print(id)\n",
    "    #             for index, row in pf_res.iterrows():\n",
    "    #                  pf_res.at[index, 'ROI'] = ROI\n",
    "                fpf.iloc[i, -1] = ROI\n",
    "                \n",
    "            \n",
    "        \n",
    "\n",
    "                # pf_res.at[pl, 'ROI'] = str(ROI)\n",
    "    # df.at[index, 'new_column'] = new_value\n",
    "    print(time.time()-st)\n",
    "\n",
    "    # Results - Objects per ROI in separate CSV files\n",
    "\n",
    "    st = time.time()\n",
    "    # using list\n",
    "    results = list()\n",
    "    \n",
    "    for r in ROI_names:\n",
    "        #print('ROI IS', r, 'roi list is', ROI_names)\n",
    "        \n",
    "        results.append(fpf.loc[(fpf['ROI']==r)])\n",
    "        #print(results)\n",
    "        \n",
    "        \n",
    "\n",
    "    return (results, ROI_names, ROI_Area)\n",
    "\n",
    "def handle_csv(results, ROI_names, imagename, csv_object_reference, ROI_Area, homepath, project, subproject):\n",
    "\n",
    "        \n",
    "    fp = homepath+project+'_'+subproject+'/In_Progress/'+str(imagename)+'_'+csv_object_reference+'_results_{}.csv'.format(i)\n",
    "        \n",
    "    r['Condition']=imagename.split('_')[2]\n",
    "    if not 'S' in imagename[:4]:\n",
    "        r['Section']= imagename.split('S')[1].split('_')[0]\n",
    "    else:\n",
    "        r['Section']= imagename.split('S')[2].split('_')[0]\n",
    "    if '_' in csv_object_reference:\n",
    "        r['Object']=csv_object_reference.split('_')[0]\n",
    "    else:\n",
    "        r['Object']=csv_object_reference.split('.')[0]\n",
    "    r['Measurement']=imagename[:4]\n",
    "    r['Region_Area']=a\n",
    "  \n",
    "        #print(r.head(5))\n",
    "    \n",
    "    return r, fp\n",
    "\n",
    "def harmony_omero_output (user, password, host, datasets, directory):\n",
    "    \n",
    "    all_data = []\n",
    "    pathlist=[]\n",
    "    csv_list=[]\n",
    "    joined_data=[]\n",
    "    \n",
    "\n",
    "    with BlitzGateway(user , password, host=host, port=4064) as conn:\n",
    "\n",
    "            conn.connect()\n",
    "            conn.c.enableKeepAlive(300) # 120-300? Certainly less than 600\n",
    "            print('connected')\n",
    "            #print(user, password, host, datasets, directory)\n",
    "\n",
    "            for dataset in datasets:\n",
    "                for image in conn.getObjects('Image', opts={'dataset': dataset}):\n",
    "                    #print(image)\n",
    "                    imageId = image.getid()\n",
    "                    imagename=image.getName()\n",
    "                    dataset=image.getParent()\n",
    "                    dataseto=dataset.getName()\n",
    "                    datasetname=dataseto.upper()\n",
    "                    this_iteration = {'imageId': imageId, 'imagename': imagename, 'dataset': dataset, 'datasetname': datasetname}\n",
    "                    all_data.append(this_iteration)\n",
    "                    #print (all_data)\n",
    "                   \n",
    "                    \n",
    "                    #print('all data length is:', len(all_data))\n",
    "                    #all data is a list of dictionaries, 8 dictionaries, each corresponding to one image, 8 images in total.\n",
    "                    #print (imageId, imagename, datasetname, dataset.getid())\n",
    "                    \n",
    "    ## NEED TO FIND AN EFFICIENT WAY TO CHOOSE WHICH MEASUREMENTS FROM THE DIRECTORY TO SEGMENT\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        #print(filename)\n",
    "        if 'Measurement' in filename:\n",
    "            # loops through the two measurements x2\n",
    "            for file in os.listdir(directory+filename):\n",
    "                regexmatch = re.match('[a-zA-Z]*\\d', file)\n",
    "                if regexmatch:\n",
    "                    print('regex match', regexmatch[0])\n",
    "                    csv_directory=directory+filename+'/'+regexmatch[0]+'/'\n",
    "                    #print(csv_directory)\n",
    "                \n",
    "                \n",
    "                \n",
    "                    for csv in os.listdir(csv_directory):\n",
    "                        #print('joined data directory:', csv_directory)\n",
    "                                #loops though all csvs for each measurement (many)\n",
    "                        #print(csv)\n",
    "                        if 'Objects_Population' in csv:\n",
    "                            #print(csv_list)\n",
    "                            csv_object_reference = csv[21:]\n",
    "\n",
    "                            csv_slideID=filename[:4].upper()\n",
    "                            csvpath=\"\".join([csv_directory, csv])\n",
    "                                        #path = {'csv_path': csvpath}\n",
    "                            csvdata={'csvpath': csvpath, 'csvname': csv_object_reference, 'slideId':csv_slideID}\n",
    "                            csv_list.append(csvdata)\n",
    "\n",
    "    \n",
    "    \n",
    "        #print('csvs here:', csv_list)\n",
    "                #creates list of all csvpaths in both measurements x14\n",
    "                \n",
    "    for index, csv in enumerate(csv_list, start=0):\n",
    "        \n",
    "                    #assigns number to each list item\n",
    "        #print('enumerated1')\n",
    "        for index, image in enumerate(all_data, start=0):\n",
    "            #print('enumerate 2: all data enumerated', index, image)\n",
    "            for csvk, csvv in csv.items():\n",
    "                \n",
    "                for imagek, imagev in image.items():\n",
    "                    \n",
    "                    if csvv == imagev:\n",
    "                        #print(csvv, 'is the same as', imagev)\n",
    "                        newdict={**csv, **image}\n",
    "                        joined_data.append(newdict)\n",
    "    #print(joined_data)\n",
    "                        \n",
    "    return(joined_data)\n",
    "\n",
    "def concat_data(analysis_file_list, directory):\n",
    "    list2concat=[]\n",
    "    \n",
    "    for analysis_filename in analysis_file_list:\n",
    "        df=pd.read_csv(directory+'/'+analysis_filename, delimiter='\\t', engine='python')\n",
    "        #if analysis type in csv name\n",
    "        list2concat.append(df)\n",
    "        slideId=analysis_filename[:4]\n",
    "        # concatdata=slideId+'_'+analysis_name\n",
    "    \n",
    "    concatdata=pd.concat(list2concat, ignore_index=True)           \n",
    "    return concatdata\n",
    "\n",
    "def concatnormfiles(inputdir, outputdir, project, subproject):\n",
    "    file_list=[]\n",
    "\n",
    "# List of files in dir\n",
    "    workingdir='/home/staffan/Code_KR/'+project+'_'+subproject+'/In_Progress/'\n",
    "    print(workingdir)\n",
    "\n",
    "    file_l=os.listdir(workingdir)\n",
    "    for i in file_l:\n",
    "        if '.csv' in i:\n",
    "            file_list.append(i)\n",
    "            \n",
    "    regex = '(\\_\\d\\d\\_|\\_\\D\\D\\_|\\_\\D\\D\\D\\_)([a-zA-Z0-9\\_\\+]*)'\n",
    "\n",
    "    # Get everything after --_ or ++_ and before .txt\n",
    "    # We use this to create ana_list, uses a regex to grab everything between those strings.\n",
    "    analysis_list = [re.search(regex, filename).group(2) for filename in file_list]\n",
    "    # Make the list only populated with unique items\n",
    "    analysis_list = list(set(analysis_list))\n",
    "    print('1', analysis_list)\n",
    "\n",
    "    analysis_files_dict = {}\n",
    "    for analysis_item in analysis_list:\n",
    "        print('2', analysis_item, analysis_list)\n",
    "        analysis_files_dict[analysis_item] = []\n",
    "        for filename in file_list:\n",
    "            if analysis_item in filename:\n",
    "                print('3', filename)\n",
    "                analysis_files_dict[analysis_item].append(filename)\n",
    "                #print('4', analysis_files_dict)\n",
    "\n",
    "    for analysis_item, analysis_filelist in analysis_files_dict.items():\n",
    "        print('4', analysis_item)\n",
    "        output = concat_data(analysis_filelist, workingdir)\n",
    "        output=transform_px_to_um2 (output)\n",
    "        fp=outputdir+project+'/'+subproject+'/ObjectResults/'+project+'_'+subproject+'_{}.csv'.format(analysis_item)\n",
    "        \n",
    "        print('5', analysis_item)\n",
    "        output = output[~output.index.duplicated()]\n",
    "        output.to_csv(fp, sep='\\t', encoding='utf-8')\n",
    "        print('6', fp)\n",
    "                                                                                    \n",
    "    return output, fp\n",
    "    \n",
    "def transform_px_to_um2 (output):\n",
    "    \n",
    "    for col in output.columns:\n",
    "            if 'Number of Spots per Area' in col:\n",
    "                output.loc[output[col] > 0, col] = (output[col]*44.7)\n",
    "            if 'Region_Area' in col:\n",
    "                output.loc[output[col] > 0, col] = round((output[col]/44.7), 2)\n",
    "                \n",
    "    return output\n",
    "    \n",
    "def setupfolders (project, subproject, workingdir, outputdir):\n",
    "    outputfolderlist= ['CellCountResults', 'Figures', 'ObjectResults']\n",
    "    \n",
    "    for direc in outputfolderlist: \n",
    "        path=os.path.join(outputdir+project+'/'+subproject+'/'+direc)\n",
    "        if os.path.exists(path):\n",
    "            print('exists')\n",
    "        else:\n",
    "            os.mkdir(path)\n",
    "    path2=os.path.join(workingdir+project+'_'+subproject+'/'+'In_Progress')\n",
    "    if os.path.exists(path2):\n",
    "            print('exists')\n",
    "    else: \n",
    "        os.makedirs(path2)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "###PARAMETERS\n",
    "#datasets= dataset ids in omero\n",
    "datasets=[876]\n",
    "#workingdir= linux user home\n",
    "workingdir='/home/staffan/Code_KR/'\n",
    "outputdir='/mnt/Labox/Katherine/'\n",
    "### outputdirectory== dir/project/subproject/subsub i.e. ddOPC/2/E1\n",
    "### working == wdir/project_subproject i.e ddOPC_2\n",
    "project='Sonic'\n",
    "subproject='33'\n",
    "user='Katherine'\n",
    "password= 'Katherine'\n",
    "host='localhost'\n",
    "#subsubs=['E5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected\n",
      "regex match Evaluation1\n",
      "Getting metadata from OMERO...\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "data/OMERO/ManagedRepository/Katherine_152/2022-03/31/19-19-01.957/A4_F5P0T0.ome.tiff\n",
      "Found the images\n",
      "dataset is: BNIS json is\n",
      "metapath found /mnt/Labox/Katherine/Sonic/33/Ref/BNIS__2022-03-25T09_38_18-Measurement 2/Images/FieldTables.json\n",
      "0.5353217124938965\n",
      "Reading images, identifying clusters (sections)...\n",
      "BNIS\n",
      "ref xml is: /mnt/Labox/Katherine/Sonic/33/Ref/BNIS__2022-03-25T09_38_18-Measurement 2/Images/Index.ref.xml\n",
      "27.590454578399658\n",
      "...Done\n",
      "Converting ROI coordinates...\n",
      "12.981391191482544\n",
      "All Done!\n",
      "0.0015206336975097656\n",
      "0.21892905235290527\n",
      "0.35628581047058105\n",
      "Getting metadata from OMERO...\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "data/OMERO/ManagedRepository/Katherine_152/2022-03/31/19-17-20.108/A4_F4P0T0.ome.tiff\n",
      "Found the images\n",
      "dataset is: BNIS json is\n",
      "metapath found /mnt/Labox/Katherine/Sonic/33/Ref/BNIS__2022-03-25T09_38_18-Measurement 2/Images/FieldTables.json\n",
      "0.4887204170227051\n",
      "Reading images, identifying clusters (sections)...\n",
      "BNIS\n",
      "ref xml is: /mnt/Labox/Katherine/Sonic/33/Ref/BNIS__2022-03-25T09_38_18-Measurement 2/Images/Index.ref.xml\n",
      "23.29262638092041\n",
      "...Done\n",
      "Converting ROI coordinates...\n",
      "12.876281976699829\n",
      "All Done!\n",
      "0.002499103546142578\n",
      "0.17487668991088867\n",
      "0.2672462463378906\n",
      "Getting metadata from OMERO...\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "data/OMERO/ManagedRepository/Katherine_152/2022-03/31/19-13-35.148/A4_F3P0T0.ome.tiff\n",
      "Found the images\n",
      "dataset is: BNIS json is\n",
      "metapath found /mnt/Labox/Katherine/Sonic/33/Ref/BNIS__2022-03-25T09_38_18-Measurement 2/Images/FieldTables.json\n",
      "0.5024752616882324\n",
      "Reading images, identifying clusters (sections)...\n",
      "BNIS\n",
      "ref xml is: /mnt/Labox/Katherine/Sonic/33/Ref/BNIS__2022-03-25T09_38_18-Measurement 2/Images/Index.ref.xml\n",
      "51.97559857368469\n",
      "...Done\n",
      "Converting ROI coordinates...\n",
      "13.565261840820312\n",
      "All Done!\n",
      "0.0014002323150634766\n",
      "0.18251538276672363\n",
      "2.2751107215881348\n",
      "Getting metadata from OMERO...\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "data/OMERO/ManagedRepository/Katherine_152/2022-03/31/19-10-49.302/A4_F2P0T0.ome.tiff\n",
      "Found the images\n",
      "dataset is: BNIS json is\n",
      "metapath found /mnt/Labox/Katherine/Sonic/33/Ref/BNIS__2022-03-25T09_38_18-Measurement 2/Images/FieldTables.json\n",
      "5.501356363296509\n",
      "Reading images, identifying clusters (sections)...\n",
      "BNIS\n",
      "ref xml is: /mnt/Labox/Katherine/Sonic/33/Ref/BNIS__2022-03-25T09_38_18-Measurement 2/Images/Index.ref.xml\n",
      "44.16188073158264\n",
      "...Done\n",
      "Converting ROI coordinates...\n",
      "11.886783361434937\n",
      "All Done!\n",
      "0.001463174819946289\n",
      "0.18436551094055176\n",
      "2.8068530559539795\n",
      "/home/staffan/Code_KR/Sonic_33/In_Progress/\n",
      "1 ['Microglia']\n",
      "2 Microglia ['Microglia']\n",
      "3 BNIS_S2_++_Microglia.txt_results_L2_3.csv\n",
      "3 BNIS_S1_--_Microglia.txt_results_L4.csv\n",
      "3 BNIS_S2_--_Microglia.txt_results_L6.csv\n",
      "3 BNIS_S1_--_Microglia.txt_results_L5.csv\n",
      "3 BNIS_S1_++_Microglia.txt_results_L1.csv\n",
      "3 BNIS_S1_++_Microglia.txt_results_L4.csv\n",
      "3 BNIS_S2_++_Microglia.txt_results_L5.csv\n",
      "3 BNIS_S2_++_Microglia.txt_results_L1.csv\n",
      "3 BNIS_S1_++_Microglia.txt_results_L6.csv\n",
      "3 BNIS_S1_--_Microglia.txt_results_L1.csv\n",
      "3 BNIS_S2_--_Microglia.txt_results_L1.csv\n",
      "3 BNIS_S2_--_Microglia.txt_results_L2_3.csv\n",
      "3 BNIS_S2_--_Microglia.txt_results_L5.csv\n",
      "3 BNIS_S1_--_Microglia.txt_results_L2_3.csv\n",
      "3 BNIS_S2_++_Microglia.txt_results_L6.csv\n",
      "3 BNIS_S2_++_Microglia.txt_results_L4.csv\n",
      "3 BNIS_S1_++_Microglia.txt_results_L5.csv\n",
      "3 BNIS_S1_--_Microglia.txt_results_L6.csv\n",
      "3 BNIS_S2_--_Microglia.txt_results_L4.csv\n",
      "3 BNIS_S1_++_Microglia.txt_results_L2_3.csv\n",
      "4 Microglia\n",
      "5 Microglia\n",
      "6 /mnt/Labox/Katherine/Sonic/33/ObjectResults/Sonic_33_Microglia.csv\n",
      "concat data went to: /mnt/Labox/Katherine/Sonic/33/ObjectResults/Sonic_33_Microglia.csv\n"
     ]
    }
   ],
   "source": [
    "#for subsub in subsubs:\n",
    "harmonydir=outputdir+project+'/'+subproject+'/Evaluations/'\n",
    "\n",
    "workingdir='/home/staffan/Code_KR/'\n",
    "setupfolders(project, subproject, workingdir, outputdir)\n",
    "\n",
    "jdata = harmony_omero_output(user, password, host, datasets, harmonydir)\n",
    "\n",
    "for jdatadict in jdata:\n",
    "    #print('rawdata is', jdatadict['csvpath'])\n",
    "    csv, ROI_names, pol_area =segment_data (user, password, jdatadict['imagename'], jdatadict['imageId'], jdatadict['csvpath'], jdatadict['csvname'])\n",
    "\n",
    "    for r, i, a in zip(csv, ROI_names, pol_area):\n",
    "        csv_output,filepath=handle_csv (csv, ROI_names, jdatadict['imagename'], jdatadict['csvname'], pol_area, workingdir, project, subproject)\n",
    "        csv_output.to_csv(filepath, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "objectresults, resultspath = concatnormfiles(workingdir, outputdir, project, subproject)\n",
    "print('concat data went to:', resultspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data, metadata):\n",
    "    data = data.merge(metadata, on='cell_id')\n",
    "    data = data.dropna()  # Handle missing values as needed\n",
    "    return data\n",
    "\n",
    "def feature_extraction(data):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data.iloc[:, :-1])  # Assuming the last column is the label\n",
    "    pca = PCA(n_components=10)\n",
    "    features = pca.fit_transform(scaled_data)\n",
    "    return features\n",
    "\n",
    "def prepare_data_for_ml(features, labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_rf_classifier(X_train, y_train):\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def evaluate_classifier(clf, X_test, y_test):\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return accuracy, cm\n",
    "\n",
    "def build_deep_learning_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_dim))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_deep_learning_model(model, X_train, y_train, epochs=100, batch_size=32):\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_file_path = \"your_data_file.csv\"\n",
    "    metadata_file_path = \"your_metadata_file.csv\"\n",
    "\n",
    "    # Load and preprocess data\n",
    "    data = load_data(data_file_path)\n",
    "    metadata = load_data(metadata_file_path)\n",
    "    preprocessed_data = preprocess_data(data, metadata)\n",
    "\n",
    "    # Feature extraction\n",
    "    features = feature_extraction(preprocessed_data)\n",
    "    labels = preprocessed_data['label']  # Assuming a column named 'label' contains the class labels\n",
    "\n",
    "    # Prepare data for machine learning\n",
    "    X_train, X_test, y_train, y_test = prepare_data_for_ml(features, labels)\n",
    "\n",
    "    # Train and evaluate a Random Forest classifier\n",
    "    rf_clf = train_rf_classifier(X_train, y_train)\n",
    "    rf_accuracy, rf_cm = evaluate_classifier(rf_clf, X_test, y_test)\n",
    "    print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "    print(f\"Random Forest Confusion Matrix: {rf_cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import measure\n",
    "from skimage.segmentation import clear_border\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def load_image_data(image_folder, image_files):\n",
    "    images = []\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "def get_morphological_features(images):\n",
    "    features = []\n",
    "    for image in images:\n",
    "        # Thresholding\n",
    "        _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        # Clear border artifacts\n",
    "        cleared = clear_border(thresh)\n",
    "\n",
    "        # Label connected regions\n",
    "        labels = measure.label(cleared)\n",
    "\n",
    "        # Extract region properties\n",
    "        props = measure.regionprops(labels)\n",
    "\n",
    "        # Calculate morphological features\n",
    "        areas = [prop.area for prop in props]\n",
    "        perimeters = [prop.perimeter for prop in props]\n",
    "        eccentricities = [prop.eccentricity for prop in props]\n",
    "\n",
    "        features.append([np.mean(areas), np.mean(perimeters), np.mean(eccentricities)])\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocess_images(images, img_size):\n",
    "    preprocessed = [cv2.resize(img, img_size) for img in images]\n",
    "    preprocessed = np.array(preprocessed).reshape(-1, img_size[0], img_size[1], 1)\n",
    "    preprocessed = preprocessed / 255.0\n",
    "    return preprocessed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_folder = \"your_image_folder\"\n",
    "    image_files = os.listdir(image_folder)\n",
    "\n",
    "    images = load_image_data(image_folder, image_files)\n",
    "\n",
    "    # Calculate morphological features\n",
    "    morph_features = get_morphological_features(images)\n",
    "    print(\"Morphological features: \", morph_features)\n",
    "\n",
    "    # CNN feature engineering\n",
    "    img_size = (64, 64)\n",
    "    preprocessed_images = preprocess_images(images, img_size)\n",
    "    labels = np.array([0, 1])  # Adjust according to your dataset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(preprocessed_images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    cnn_model = build_cnn_model((img_size[0], img_size[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "from skimage.segmentation import clear_border\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_image(image, threshold_value):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    return thresh\n",
    "\n",
    "def split_cells(thresh_image, min_size=100, max_size=5000):\n",
    "    cleared = clear_border(thresh_image)\n",
    "    labels = measure.label(cleared)\n",
    "\n",
    "    individual_cells = []\n",
    "\n",
    "    for region in measure.regionprops(labels):\n",
    "        if min_size <= region.area <= max_size:\n",
    "            cell = thresh_image[region.bbox[0]:region.bbox[2], region.bbox[1]:region.bbox[3]]\n",
    "            individual_cells.append(cell)\n",
    "    \n",
    "    return individual_cells\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"your_microscopy_image_path.jpg\"\n",
    "    image = cv2.imread(image_path)\n",
    "    threshold_value = 128  # Adjust based on fluorescence intensity\n",
    "    min_size = 100  # Minimum cell size (adjust as needed)\n",
    "    max_size = 5000  # Maximum cell size (adjust as needed)\n",
    "\n",
    "    thresh_image = preprocess_image(image, threshold_value)\n",
    "    individual_cells = split_cells(thresh_image, min_size, max_size)\n",
    "\n",
    "    # Display individual cells\n",
    "    for i, cell in enumerate(individual_cells):\n",
    "        plt.subplot(1, len(individual_cells), i+1)\n",
    "        plt.imshow(cell, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "from skimage.morphology import convex_hull_image\n",
    "\n",
    "def sholl_feature(cell):\n",
    "    cell_center = np.array(cell.shape) // 2\n",
    "    max_radius = int(np.sqrt(cell_center[0]**2 + cell_center[1]**2))\n",
    "\n",
    "    sholl_counts = []\n",
    "    for r in range(1, max_radius):\n",
    "        circle_img = np.zeros_like(cell)\n",
    "        cv2.circle(circle_img, tuple(cell_center), r, 255, 1)\n",
    "        intersections = cv2.bitwise_and(circle_img, cell)\n",
    "        intersection_count = cv2.countNonZero(intersections)\n",
    "        sholl_counts.append(intersection_count)\n",
    "\n",
    "    return sholl_counts\n",
    "\n",
    "def fmax_feature(cell):\n",
    "    contours, _ = cv2.findContours(cell, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    _, (w, h), _ = cv2.minAreaRect(largest_contour)\n",
    "    fmax = max(w, h)\n",
    "\n",
    "    return fmax\n",
    "\n",
    "def solidity_feature(cell):\n",
    "    convex_hull = convex_hull_image(cell)\n",
    "    area = np.sum(cell > 0)\n",
    "    convex_area = np.sum(convex_hull > 0)\n",
    "    solidity = area / convex_area\n",
    "\n",
    "    return solidity\n",
    "\n",
    "def feret_diameter_ratio_feature(cell):\n",
    "    contours, _ = cv2.findContours(cell, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    _, (w, h), _ = cv2.minAreaRect(largest_contour)\n",
    "    fmax = max(w, h)\n",
    "    fmin = min(w, h)\n",
    "    feret_diameter_ratio = fmax / fmin\n",
    "\n",
    "    return feret_diameter_ratio\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cell_image_path = \"your_cell_image_path.jpg\"\n",
    "    cell = cv2.imread(cell_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    _, cell = cv2.threshold(cell, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    sholl_counts = sholl_feature(cell)\n",
    "    print(\"Sholl counts:\", sholl_counts)\n",
    "\n",
    "    fmax = fmax_feature(cell)\n",
    "    print(\"Fmax:\", fmax)\n",
    "\n",
    "    solidity = solidity_feature(cell)\n",
    "    print(\"Solidity:\", solidity)\n",
    "\n",
    "    feret_diameter_ratio = feret_diameter_ratio_feature(cell)\n",
    "    print(\"Feret's Diameter Ratio:\", feret_diameter_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Include the feature calculation functions (sholl_feature, fmax_feature, solidity_feature, feret_diameter_ratio_feature) here\n",
    "\n",
    "def extract_features(cell):\n",
    "    sholl_counts = sholl_feature(cell)\n",
    "    fmax = fmax_feature(cell)\n",
    "    solidity = solidity_feature(cell)\n",
    "    feret_diameter_ratio = feret_diameter_ratio_feature(cell)\n",
    "\n",
    "    # You can modify the features array based on your needs\n",
    "    features = [fmax, solidity, feret_diameter_ratio] + sholl_counts\n",
    "\n",
    "    return features\n",
    "\n",
    "def load_cells_and_labels(cells_folder):\n",
    "    cell_files = os.listdir(cells_folder)\n",
    "    cells = []\n",
    "    labels = []\n",
    "\n",
    "    for cell_file in cell_files:\n",
    "        cell_path = os.path.join(cells_folder, cell_file)\n",
    "        cell = cv2.imread(cell_path, cv2.IMREAD_GRAYSCALE)\n",
    "        _, cell = cv2.threshold(cell, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        cells.append(cell)\n",
    "        label = 0 if 'negative' in cell_file else 1  # Adjust based on your file naming convention\n",
    "        labels.append(label)\n",
    "\n",
    "    return cells, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cells_folder = \"your_cells_folder\"\n",
    "    cells, labels = load_cells_and_labels(cells_folder)\n",
    "\n",
    "    feature_matrix = np.array([extract_features(cell) for cell in cells])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Optimize SVM model using GridSearchCV\n",
    "    param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'linear']}\n",
    "    grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters found by grid search:\", grid.best_params_)\n",
    "\n",
    "    predictions = grid.predict(X_test)\n",
    "\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Include the feature calculation functions (split_cells, sholl_feature, fmax_feature, solidity_feature, feret_diameter_ratio_feature) here\n",
    "\n",
    "def segment_rna_spots(image, threshold_value=128):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, thresh = cv2.threshold(gray_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    return thresh\n",
    "\n",
    "def rna_spot_properties(cell, rna_spots, min_spot_size=5, max_spot_size=50):\n",
    "    cell_mask = cell > 0\n",
    "    rna_spots_in_cell = rna_spots.copy()\n",
    "    rna_spots_in_cell[~cell_mask] = 0\n",
    "\n",
    "    spot_labels = measure.label(rna_spots_in_cell)\n",
    "    properties = measure.regionprops_table(spot_labels, properties=('label', 'area', 'mean_intensity', 'equivalent_diameter'))\n",
    "\n",
    "    filtered_properties = {k: [v[i] for i in range(len(properties['label'])) if min_spot_size <= properties['area'][i] <= max_spot_size] for k, v in properties.items()}\n",
    "\n",
    "    return filtered_properties\n",
    "\n",
    "def extract_features(cells, rna_spots):\n",
    "    feature_matrix = []\n",
    "\n",
    "    for cell in cells:\n",
    "        cell_properties = rna_spot_properties(cell, rna_spots)\n",
    "        num_spots = len(cell_properties['label'])\n",
    "        avg_intensity = np.mean(cell_properties['mean_intensity'])\n",
    "        avg_diameter = np.mean(cell_properties['equivalent_diameter'])\n",
    "\n",
    "        # Add more features as needed\n",
    "        features = [num_spots, avg_intensity, avg_diameter]\n",
    "        feature_matrix.append(features)\n",
    "\n",
    "    return np.array(feature_matrix)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"your_spatial_transcriptomics_image_path.jpg\"\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    cells_folder = \"your_cells_folder\"  # Contains individual cell images\n",
    "    cells, labels = load_cells_and_labels(cells_folder)\n",
    "\n",
    "    rna_spots = segment_rna_spots(image)\n",
    "    feature_matrix = extract_features(cells, rna_spots)\n",
    "\n",
    "    # Perform Spectral Clustering for dimensionality reduction\n",
    "    reduced_dim = 3\n",
    "    clustering = SpectralClustering(n_clusters=reduced_dim, affinity='nearest_neighbors', random_state=42)\n",
    "    reduced_features = clustering.fit_transform(feature_matrix)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reduced_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train and test XGBoost classifier\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Classification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
